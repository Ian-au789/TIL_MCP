import requests
import httpx
import json
from typing import List, Dict, Any, Optional

# MCP ì„œë²„ì—ì„œ íˆ´ ì •ì˜ë¥¼ ìˆ˜ì§‘í•˜ëŠ” í•¨ìˆ˜
def gather_mcp_tools(server_urls: List[str]) -> List[Dict[str, Any]]:
    tools = []
    for base_url in server_urls:
        try:
            res = requests.post(f"{base_url}/tools/define", timeout=5)
            res.raise_for_status()
            for tool in res.json().get("tools", []):
                tools.append({"server": base_url, **tool})
        except Exception as e:
            print(f"âš ï¸ {base_url} ì ‘ì† ì‹¤íŒ¨: {e}")
    return tools

# ì´ë¦„ìœ¼ë¡œ íˆ´ ì„ íƒí•˜ê¸°
def select_tool_by_name(tools: List[Dict[str, Any]], target_name: str) -> Optional[Dict[str, Any]]:
    for tool in tools:
        if tool.get("name") == target_name:
            return tool
    return None

# MCP íˆ´ ì‹¤í–‰ í•¨ìˆ˜
def run_tool(tool: Dict[str, Any], arguments: Dict[str, Any]) -> Dict[str, Any]:
    try:
        res = requests.post(
            f"{tool['server']}/tools/call",
            json={"name": tool["name"], "arguments": arguments},
            timeout=30
        )
        res.raise_for_status()
        return res.json()
    except Exception as e:
        return {"error": str(e)}

# LLM í˜¸ì¶œ í•¨ìˆ˜ (Mistral via Ollama)
def call_llm_with_prompt(prompt: str) -> str:
    payload = {
        "model": "mistral:latest",
        "prompt": prompt,
        "stream": False
    }
    try:
        response = httpx.post("http://localhost:11434/api/generate", json=payload, timeout=30)
        response.raise_for_status()
        return response.json().get("response", "").strip()
    except Exception as e:
        return f"LLM í˜¸ì¶œ ì˜¤ë¥˜: {e}"

# ì‚¬ìš©ì ìš”ì²­ê³¼ íˆ´ ëª©ë¡ì„ ê¸°ë°˜ìœ¼ë¡œ LLMì—ê²Œ íˆ´ ì„ íƒ ìš”ì²­
def ask_llm_to_select_tool(prompt: str, tools: List[Dict[str, Any]]) -> Optional[Dict[str, Any]]:
    tool_descriptions = "\n".join([
        f"- {tool['name']}: {tool['description']}" for tool in tools
    ])

    full_prompt = f"""
ë‹¤ìŒì€ ì‚¬ìš© ê°€ëŠ¥í•œ íˆ´ ëª©ë¡ì…ë‹ˆë‹¤:

{tool_descriptions}

ì‚¬ìš©ìì˜ ìš”ì²­:
\"{prompt}\"

ìœ„ ìš”ì²­ì„ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ ê°€ì¥ ì ì ˆí•œ íˆ´ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ê³ , í•„ìš”í•œ argumentsë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•´ ì£¼ì„¸ìš”.

ì¶œë ¥ í˜•ì‹ ì˜ˆ:
{{
  "name": "commit_if_needed",
  "arguments": {{}}
}}

ì˜¤ì§ JSON í˜•ì‹ë§Œ ì¶œë ¥í•´ ì£¼ì„¸ìš”.
"""
    raw_response = call_llm_with_prompt(full_prompt)

    try:
        return json.loads(raw_response)
    except Exception as e:
        print(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
        print("ğŸ” LLM ì‘ë‹µ ì›ë¬¸:", raw_response)
        return None

# ì „ì²´ ì‹¤í–‰ íë¦„
def main():
    print("ğŸš€ MCP í´ë¼ì´ì–¸íŠ¸ ì‹œì‘ë¨")
    mcp_servers = [
        "https://63b8-210-217-23-139.ngrok-free.app"
    ]

    tools = gather_mcp_tools(mcp_servers)

    user_prompt = "ì˜¤ëŠ˜ ì»¤ë°‹ì´ ì—†ë‹¤ë©´ ìë™ìœ¼ë¡œ ì»¤ë°‹í•˜ê³  í‘¸ì‹œí•´ì¤˜, ì‚¬ìš© ê°€ëŠ¥í•œ ì—¬ëŸ¬ê°œì˜ íˆ´ ì¤‘ì— ìš°ì„ ì ìœ¼ë¡œ batch_commitì„ ì‚¬ìš©í•´ì„œ ì»¤ë°‹í•˜ë„ë¡ í•´ë´"
    plan = ask_llm_to_select_tool(user_prompt, tools)

    if plan:
        selected_tool = select_tool_by_name(tools, plan["name"])
        if selected_tool:
            result = run_tool(selected_tool, plan["arguments"])
            print("\nâœ… íˆ´ ì‹¤í–‰ ê²°ê³¼:", result)
        else:
            print("âŒ ì„ íƒëœ íˆ´ì„ MCP ì„œë²„ ëª©ë¡ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    else:
        print("âŒ LLMìœ¼ë¡œë¶€í„° ì ì ˆí•œ íˆ´ í”Œëœì„ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
